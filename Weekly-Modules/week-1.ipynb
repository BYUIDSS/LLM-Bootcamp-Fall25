{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - LLM Bootcamp\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Create a simple custom chat using API calls\n",
    "- Get a Gemini API key\n",
    "- Introduce LLM concepts and services\n",
    "- Start exploring LangChain and LLM concepts generally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture (10 min): Why, What, and How?\n",
    "\n",
    "- Why code custom LLM agents?\n",
    "- What is an API and why use it?\n",
    "    - API key safety\n",
    "    \n",
    "- Brief course overview\n",
    "    - Ask for anything they want to cover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini API: Getting Started\n",
    "\n",
    "API keys are coveted (especially for LLM's). To keep yours safe, store it in the `.env` file in the main directory. This environment variable file stays local to your machine and will not be pushed to github (because the `.gitignore` file says to ignore it).\n",
    "\n",
    "The code below reads the environment variables into the current session. Be sure to save the `.env` file before running it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs the dependencies you need\n",
    "%pip install -q -U langchain langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving your API key, you can run the below code to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste code here\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "\n",
    "model.invoke(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STOP\n",
    "\n",
    "If you got things working, help your neighbor get there too. \n",
    "\n",
    "Then you can have fun with the next part together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Basics\n",
    "\n",
    "Start going through [this tutorial](https://docs.langchain.com/oss/python/langchain/messages) from LangChain. Stop after reading the section \"AI Message\".\n",
    "\n",
    "In the space below, write down what you want to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STOP\n",
    "\n",
    "Again, make sure those around you are caught up. Teach and learn from them.\n",
    "\n",
    "Then you can have fun with the next part together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Around\n",
    "\n",
    "Now that you have the basics, it is time to explore! Below are some ideas, but you can branch out and tackle anything you'd like. Tell the people around you about what you are learning while you learn it. It will help you remember, and they may be interested, too.\n",
    "\n",
    "Here are some exploration options here in the notebook:\n",
    "\n",
    "- **Prompt/Context Engineering**: This is the meat of agents and customizing LLM's. Learn how to do it well.\n",
    "- **LangChain**: Keep going, either by changing things in the code or learning about tool calling in the same tutorial\n",
    "    - **model**: Try changing the model. Each model has a unique text string that can be found in the provider's docs. For Gemini, that is [here](https://ai.google.dev/gemini-api/docs/models). \n",
    "    - **prompt**: Adjust the prompt in `contents`. See how small changes can effect the output (sometimes dramatically).\n",
    "    - **chaining**: Try taking the ouput from the previous call and passing it as the contents for another call.\n",
    "    - **tool calling**: This is where the rubber starts to meet the road and get exciting. You can continue the previous tutorial for this.\n",
    "- **Chainlit**: We will go over this in week 6, but if you are excited to use a nice interface, you can try out Chainlit. While it does make things simpler, it still requires knowing some web development concepts such as stateful development and async functionality.\n",
    "- **Visual Editors**: Python not your thing? You can try some visual editors. There is a guide under the \"Explore\" folder in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "If you think you understand prompt engineering, consider the following quote speaking for more complicated agent systems.\n",
    "\n",
    "> The prompts for deep agents often span hundreds if not thousands of lines — usually containing a general persona, instructions on calling tools, important guidelines, and few-shot examples.\n",
    "> \\- [LangChain Blog](https://www.blog.langchain.com/debugging-deep-agents-with-langsmith/)\n",
    "\n",
    "Prompt engineering is the real meat of customizing LLM's. Consider the impact of changing just a few words. Run these a few times, and try intentional word changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "prompt = \"What did you have for breakfast today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arnold 1\n",
    "model.invoke(\"You are Arnold Schwarzenegger. \" + prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arnold 2\n",
    "model.invoke(\"Act like Arnold Schwarzenegger. \" + prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arnold 3\n",
    "model.invoke(\"Speak in the tone of Arnold Schwarzenegger. \" + prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arnold 4\n",
    "model.invoke(\"You are Arnold Schwarzenegger. Respond briefly.\" + prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the best prompting advice is: be specific. It often helps to give examples.\n",
    "\n",
    "To get a good feel of prompting best practices and types of prompts, look at these resources:\n",
    "\n",
    "- [Claude docs](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices)\n",
    "- [Gemini docs](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n",
    "- [Gemini prompting with files](https://ai.google.dev/gemini-api/docs/files#prompt-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_instr = \"You are Arnold Schwarzenegger\"\n",
    "\n",
    "prompt = \"What did you have for breakfast today?\"\n",
    "\n",
    "model.invoke([\n",
    "    SystemMessage(system_instr),\n",
    "    HumanMessage(prompt)]).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Engineering\n",
    "\n",
    "But perhaps a better way to think about this is context engineering. Consider this quote:\n",
    "\n",
    "> Why the shift from “prompts” to “context”? Early on, developers focused on phrasing prompts cleverly to coax better answers. But as applications grow more complex, it’s becoming clear that **providing complete and structured context** to the AI is far more important than any magic wording.\n",
    "> \\- [Harrison's Hot Takes](https://www.blog.langchain.com/the-rise-of-context-engineering/)\n",
    "\n",
    "It may be worth giving the above article a read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on LangChain\n",
    "\n",
    "You may have noticed The model's response was more than just text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gemini-2.5-flash-lite\", model_provider=\"google-genai\")\n",
    "\n",
    "response = model.invoke(\"Be an angsty teen\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access just the text response with .content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...But there is a lot of other useful information, too. To see everything you can access with dot notation, you can use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some items are nested, like 'usage_metadata', and you may have to use bracket notation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata['input_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "\n",
    "See what happens if you take the above `response.content` and throw it into another model call. This is called chaining. What use cases can you think of for it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    model_provider=\"google_genai\",\n",
    "    temperature=0,\n",
    "    max_tokens=50,\n",
    "    timeout=60,\n",
    "    max_retries=3)\n",
    "\n",
    "model.invoke(\"Hello world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
